# # Thesis application
# # November 15, 2017 
# # Estimating the probability of causation for the RCT described (and with data) here 
# # https://www.povertyactionlab.org/evaluation/cleaning-springs-kenya
# 
# Packages
library(devtools) 
library(npcausal)
library(readstata13)
library(doParallel)
library(randomForest)
library(gmodels)
library(npcausal)
expit = function(x){ return( exp(x)/(1+exp(x)) ) }
options(warn=-1)

# Working directory
setwd("/Users/mariacuellar/Desktop/CMU/Papers/2nd Heinz paper:ADA/Shaken Baby Syndrome/THESIS/Data leads/JPAL/Springs/dta/")

# Load data
dat1 = read.dta13("reg_data_children_Aug2010.dta", missing.type = TRUE)
dim(dat1)
# dat2 = dat1[,grepl("problem_weight", names(dat1), fixed=TRUE)]


# Sample restricted to children under age three at baseline (in 2004) and children born since 2004
# /*DEFINE SAMPLE OF INTEREST*/
#   
#   *WE EXCLUDE ALL KIDS IN HOUSEHOLDS THAT RECEIVED WG IN HM3, AS WELL AS KIDS NOT AT ITT SPRINGS OR KIDS NOT IN THE HOUSEHOLD SAMPLE 
# *CHILDREN ARE INCLUDED IN THE REGRESSION SAMPLE IF THEY ARE AGE 3 OR UNDER AT BASELINE OR AGE 3 OR UNDER WHEN THEY JOIN THE SAMPLE IN LATER ROUNDS; 
# *WE ALSO EXCLUDE CHILD-ROUND OBSERVATIONS WHERE THE ANTHROPOMETRIC OR AGE DATA IS FLAGGED AS HAVING A SERIOUS ERROR;
# *FINALLY, OBSERVATIONS MISSING DATA ON WHETHER THE HOUSEHOLD USES THE ICS SPRING EXCLUSIVELY OR USES MULTIPLE SPRINGS ARE ALSO EXCLUDED FROM THE REGRESSIONS;

dat2 = dat1[which( dat1$multiusers_l_base==0 | is.na(dat1$multiusers_l_base==0) ),]
dat3 = dat2[which( dat2$height_outlier_severe==0 | is.na(dat2$height_outlier_severe) ),]
dat4 = dat3[which( dat3$problem_weight==0 | is.na(dat3$problem_weight) ),]
dat5 = dat4[which( dat4$problem_bmi==0 | is.na(dat4$problem_bmi) ),]
dat6 = dat5[which( dat5$flag_age==0 | is.na(dat5$flag_age) ),]
dat7 = dat6[which( dat6$problem_age==0 | is.na(dat6$problem_age) ),]
dat8 = dat7[which( dat7$base_age<=3 | is.na(dat7$base_age) ),]

dat = dat8
dim(dat) # [1] 22620  4146


# Also have behaviors (handwashing), household water collection and treatment behavior, and socioeconomic sta- tus
# Make my variables
A = ifelse(dat$evertreat=="TREAT", 0, 1) # Treatment. What about this var? ba_tc
Y = dat$c14_d_child_diarrhea # Diarrhea in past week. Diarrhea defined as three or more “looser than normal” stools within 24 hours at any time in the past week
X1 = dat$c13_c_child_gender # gender
X2 = dat$base_age # age at baseline
X3 = dat$momeduc_orig # Mother's years of education
X4.0 = dat$splnecmpn_base # Baseline water quality, ln(spring water E. coli MPN) # High quality water: MPN <=1, High or moderate quality: MPN < 126, water is poor quality: MPN = 126-1000
X5 = dat$e1_iron_roof_base # Home has iron roof indicator
X6.0 = dat$hygiene_know_base # Mother's hygiene knowledge at baseline. average of demeaned sum of number of correct responses given to the open-ended question “to your knowledge, what can be done to prevent diarrhea?
X7.0 = dat$latrine_density_base # Baseline latrine density
X8.0 = dat$numkids_base # Number of children under 12 living at home
samplesize = length(A)


# Undoing the de-meaned variables
X4.1 = X4.0 - (min(X4.0[X4.0<0], na.rm = TRUE) + max(X4.0[X4.0<0], na.rm = TRUE))
X4 = X4.1 - min(X4.1, na.rm = TRUE)

X6.1 = X6.0 - (min(X6.0[X6.0<0], na.rm = TRUE) + max(X6.0[X6.0<0], na.rm = TRUE))
X6 = X6.1 - min(X6.1, na.rm = TRUE)

X7.1 = X7.0 - (min(X7.0[X7.0<0], na.rm = TRUE) + max(X7.0[X7.0<0], na.rm = TRUE))
X7 = X7.1 - min(X7.1, na.rm = TRUE)

X8.1 = X8.0 - (min(X8.0[X8.0<0], na.rm = TRUE) + max(X8.0[X8.0<0], na.rm = TRUE))
X8 = X8.1 - min(X8.1, na.rm = TRUE)

# Make data frame, delete all missing values
dff0 = as.data.frame(cbind(A, Y, X1, X2, X3, X4, X5, X6, X7, X8))
dff = na.omit(dff0)

nrow(dff) # [1] 2933



# EDA

# Checking crosstab for Y and A
table(A, useNA="ifany")
table(Y, useNA="ifany")
table(dff$A, dff$Y, useNA="ifany")

# Gender distribution
table(dff$X1)

# Iron roof indicator
table(dff$X5)


pdf(file="EDA.pdf", width=10, height=5)

par(mfrow=c(2,3))
# Age distribution
hist(dff$X2, col="gray", main="Age distribution", xlab="Age in months")

# Mother's education
hist(dff$X3, col="gray", main="Mother education distribution \n at baseline", xlab="Years of education")

# Water quality
hist(dff$X4, col="gray", main="Water quality at baseline", xlab="Water quality (ln(E.coli MPN))")

# Hygiene knowledge at baseline
hist(dff$X6, col="gray", main="Mother's hygiene knowledge \n at baseline", xlab="Hygiene knowledge score")

# Latrine density at baseline
hist(dff$X7, col="gray", main="Latrine density near household \n at baseline", xlab="Latrine density (out of 0.6)")

# Number of children at baseline
hist(dff$X8, col="gray", main="Number of children in household \n at baseline", xlab="Number of children")

dev.off()

par(mfrow=c(1,1))

# rm(list=ls())
# setwd("/Users/mariacuellar/Desktop/CMU/Papers/2nd Heinz paper:ADA/Shaken Baby Syndrome/THESIS/Data leads/JPAL/Springs/")
# save(dff, file="dff.rda")
# load("dff.rda")

table(dff$Y)
table(dff$A)
table(dff$A, dff$Y)

dff = dff[which(dff$X2<=3),]

head(dff)

library0 = c("SL.earth","SL.gam","SL.glm","SL.glmnet", "SL.glm.interaction", "SL.mean","SL.ranger")
library1 = c("SL.gam","SL.glm")

ATE = ate(y=dff$Y, a=dff$A, x=data.frame(cbind(dff[,3:10])))
ATE[1]$res
(ATE[1]$res$est[2] - ATE[1]$res$est[1])/(ATE[1]$res$est[2]) # [1] 1.465586



# selected point to evaluate PC
mypoint = c(1, 6, 6, 4, 1, 3, .4, 5)
dff.test1 = dff[1,]
dff.test1[1, 3:10] = mypoint

# data splitting
index = sample(1:nrow(dff)) # randomize indices
dff.train = dff[index,][1:(nrow(dff)/2),] # make training set
dff.test = dff[index,][((nrow(dff)/2)+1):nrow(dff),] # make testing set
dff.test1 = dff.test[1,]
dff.test1[1,]$X1 = dff.test1[1,]$X2 = dff.test1[1,]$X3 = dff.test1[1,]$X4 = mypoint[1]

# estimating nuisance functions
PI_P_X_mu0 = glm(Y~X1+X2+X3+X4, data = dff.train[which(dff.train$A==0),], family = "poisson")
PI_P_X_mu1 = glm(Y~X1+X2+X3+X4, data = dff.train[which(dff.train$A==1),], family = "binomial")

# getting fitted values (after inverse link function)
PI_P_X_mu0_hat = predict(PI_P_X_mu0, newdata = dff.test, type="response")
PI_P_X_mu1_hat = predict(PI_P_X_mu1, newdata = dff.test, type="response")

PI_P_X_mu0_hat_mypoint = predict(PI_P_X_mu0, newdata = dff.test1, type="response")
PI_P_X_mu1_hat_mypoint = predict(PI_P_X_mu1, newdata = dff.test1, type="response")

# calculating gamma hat
PI_P_X_gammahat = 1 - PI_P_X_mu0_hat/PI_P_X_mu1_hat
PI_P_X_gammahat_mypoint = 1 - PI_P_X_mu0_hat_mypoint/PI_P_X_mu1_hat_mypoint
PI_P_X_gammahat_mypoint

  fun.PI_P_X_boot = function(xthedata){
    
    # data splitting
    index = sample(1:nrow(dff)) # randomize indices
    dff.train = dff[index,][1:(nrow(dff)/2),] # make training set
    dff.test = dff[index,][((nrow(dff)/2)+1):nrow(dff),] # make testing set
    dff.test1 = dff.test[1,]
    dff.test1[1,]$X1 = dff.test1[1,]$X2 = dff.test1[1,]$X3 = dff.test1[1,]$X4 = mypoint[1]
    
    # estimating nuisance functions
    PI_P_X_mu0 = glm(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==0),], family = "poisson")
    PI_P_X_mu1 = glm(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==1),], family = "binomial")
    
    # getting fitted values
    PI_P_X_mu0_hat_mypoint = predict(PI_P_X_mu0, newdata = dff.test1, type="response")
    PI_P_X_mu1_hat_mypoint = predict(PI_P_X_mu1, newdata = dff.test1, type="response")
    
    # calculating gamma hat
    PI_P_X_gammahat_mypoint = 1 - PI_P_X_mu0_hat_mypoint/PI_P_X_mu1_hat_mypoint
    
    return(PI_P_X_gammahat_mypoint)
  }
  
  # apply bootstrap
  bootreps=100
  PI_P_X_bootvec = 0
  foreach(b=1:bootreps, .options.multicore=list(preschedule=TRUE)) %dopar% {
  
    # randomize indices
    index.b = sample(1:nrow(dff), replace=FALSE)
    
    # select new sample of rows from dff
    newdff = dff[index.b, ]
    
    # calculating gammahat_star (star because it's from a bootstrap)
    PI_P_X_gammahat_star = fun.PI_P_X_boot(newdff)
    
    # store in a vector
    PI_P_X_bootvec[b] <- PI_P_X_gammahat_star
  }

  # standard error
  PI_P_X_gammahat_sd_mypoint = sd(unlist(PI_P_X_bootvec.1))
  
  # confidence interval lower bound and upper bound
  PI_P_X_gammahat_mypoint - 2*PI_P_X_gammahat_sd_mypoint / sqrt(dim(dff)[1])
  PI_P_X_gammahat_mypoint + 2*PI_P_X_gammahat_sd_mypoint / sqrt(dim(dff)[1])


  
  
# confidence interval
PI_P_X_ci_mypoint = paste(PI_P_X_gammahat_mypoint - 2*PI_P_X_gammahat_sd_mypoint / sqrt(dim(dff)[1]), 
                          PI_P_X_gammahat_mypoint + 2*PI_P_X_gammahat_sd_mypoint / sqrt(dim(dff)[1]), 
                          sep=", ")

# [1] "0.240194504684209, 0.242980511173077"


# Plug-in, nonparametric, untransformed X's estimator:
# RandomForest----
# Fitting model
# data splitting
index = sample(1:nrow(dff)) # randomize indices
dff.train = dff[index,][1:(nrow(dff)/2),] # make training set
dff.test = dff[index,][((nrow(dff)/2)+1):nrow(dff),] # make testing set

mypoint = c(1, 6, 6, 4, 1, 3, .4, 5)
dff.test1 = dff.test[1,]
dff.test1[1,]$X1 = mypoint[1]
dff.test1[1,]$X2 = mypoint[2]
dff.test1[1,]$X3 = mypoint[3]
dff.test1[1,]$X4 = mypoint[4]
dff.test1[1,]$X5 = mypoint[5]
dff.test1[1,]$X6 = mypoint[6]
dff.test1[1,]$X7 = mypoint[7]
dff.test1[1,]$X8 = mypoint[8]

# estimating nuisance functions
PI_N_X_mu0 = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==0),], type=regression)
PI_N_X_mu1 = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==1),], type=regression)

# Getting fitted values (after inverse link function)
PI_N_X_mu0_hat = as.numeric(predict(PI_N_X_mu0, newdata=dff.test)) # with random forest
PI_N_X_mu1_hat = as.numeric(predict(PI_N_X_mu1, newdata=dff.test))

PI_N_X_mu0_hat_mypoint = as.numeric(predict(PI_N_X_mu0, newdata=dff.test1)) # with random forest
PI_N_X_mu1_hat_mypoint = as.numeric(predict(PI_N_X_mu1, newdata=dff.test1))

# calculating gamma hat
PI_N_X_gammahat = 1 - PI_N_X_mu0_hat/PI_N_X_mu1_hat
PI_N_X_gammahat_mypoint = 1 - PI_N_X_mu0_hat_mypoint/PI_N_X_mu1_hat_mypoint

# Confidence interval
# # No valid confidence interval here.
PI_N_X_ci_mypoint = paste(NA, NA, sep=", ")




#  Influence-function-based estimator, estimate nuisance parameters with plugin nonparametric untransformed Xs ----
#  Defining the nuisance parameters
# data splitting
index = sample(1:nrow(dff)) # randomize indices
dff.train = dff[index,][1:(nrow(dff)/2),] # make training set
dff.test = dff[index,][((nrow(dff)/2)+1):nrow(dff),] # make testing set

# estimating nuisance functions
IF_N_X_mu0 = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==0),], type=regression, na.action = na.roughfix)
IF_N_X_mu1 = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==1),], type=regression, na.action = na.roughfix)
IF_N_X_pi = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train, type=regression, na.action = na.roughfix)

# Getting fitted values (after inverse link function)
IF_N_X_mu0_hat = as.numeric(predict(IF_N_X_mu0, newdata=dff.test)) # with random forest
IF_N_X_mu1_hat = as.numeric(predict(IF_N_X_mu1, newdata=dff.test))
IF_N_X_pi_hat = as.numeric(predict(IF_N_X_pi, newdata=dff.test))

IF_N_X_mu0_hat = PI_N_X_mu0_hat
IF_N_X_mu1_hat = PI_N_X_mu1_hat

# Defining my pseudo-outcome
mu0 = IF_N_X_mu0_hat
mu1 = IF_N_X_mu1_hat
pi = IF_N_X_pi_hat
A = dff$A
Y = dff$Y

IF_N_X_ystar = (1/mu1)*((mu0/mu1)*(1/pi)*A*(Y-mu1) - (1/(1-pi))*(1-A)*(Y-mu0)) + (1 - mu0/mu1)

# Fitting model
param=true_values
theY=IF_N_X_ystar

fun.obj = function(param, theY){
  
  beta1 = param[1]
  beta2 = param[2]
  beta3 = param[3]
  beta4 = param[4]
  beta5 = param[5]
  beta6 = param[6]
  beta7 = param[7]
  beta8 = param[8]
  beta = as.matrix(c(beta1, beta2, beta3, beta4, beta5, beta6, beta7, beta8))
  
  # change these for each estimator
  X = as.matrix(dff[, 3:(dim(dff)[2])])
  g = expit(X %*% beta)
  
  psi1 = mean( X[,1]*g*(1-g)*(theY - g) )
  psi2 = mean( X[,2]*g*(1-g)*(theY - g) )
  psi3 = mean( X[,3]*g*(1-g)*(theY - g) )
  psi4 = mean( X[,4]*g*(1-g)*(theY - g) )
  psi5 = mean( X[,5]*g*(1-g)*(theY - g) )
  psi6 = mean( X[,6]*g*(1-g)*(theY - g) )
  psi7 = mean( X[,7]*g*(1-g)*(theY - g) )
  psi8 = mean( X[,8]*g*(1-g)*(theY - g) )
  
  psi_norm = sqrt( psi1^2 + psi2^2 + psi3^2 + psi4^2 + psi5^2 + psi6^2 + psi7^2 + psi8^2 )
  return(-psi_norm)
}

true_values = c(.1,.1,.1,.1,.1,.1,.1,.1)
result_optim = optim(f = fun.obj, p = true_values, theY = IF_N_X_ystar) #,control=list(trace=TRUE))
betahat = result_optim$par
betahat # [1]  2.53984331  0.82933033 -1.42309767 -0.02986857  0.89654234  2.37261168  2.23507364  0.29806193
round(exp(betahat),2) # [1] 12.68  2.29  0.24  0.97  2.45 10.73  9.35  1.35

# Getting predicted values
X = as.matrix(dff[, 3:(dim(dff)[2])])
IF_N_X_gammahat = expit(X %*% betahat)
IF_N_X_gammahat_mypoint = expit(betahat[1]*mypoint[1] + betahat[2]*mypoint[2] + 
                                  betahat[3]*mypoint[3] + betahat[4]*mypoint[4] +
                                  betahat[5]*mypoint[5] + betahat[6]*mypoint[6] +
                                  betahat[7]*mypoint[7] + betahat[8]*mypoint[8])

IF_N_X_gammahat_mypoint

# # variance with bootstrap
# fun.IF_N_X_boot = function(xthedata){
#   
#   #  Defining the nuisance parameters
#   # data splitting
#   index = sample(1:nrow(dff)) # randomize indices
#   dff.train = dff[index,][1:(nrow(dff)/2),] # make training set
#   dff.test = dff[index,][((nrow(dff)/2)+1):nrow(dff),] # make testing set
#   
#   # estimating nuisance functions
#   IF_N_X_mu0 = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==0),], type=regression, na.action = na.roughfix)
#   IF_N_X_mu1 = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train[which(dff.train$A==1),], type=regression, na.action = na.roughfix)
#   IF_N_X_pi = randomForest(Y~X1+X2+X3+X4+X5+X6+X7+X8, data = dff.train, type=regression, na.action = na.roughfix)
#   
#   # Getting fitted values (after inverse link function)
#   IF_N_X_mu0_hat = as.numeric(predict(IF_N_X_mu0, newdata=dff.test)) # with random forest
#   IF_N_X_mu1_hat = as.numeric(predict(IF_N_X_mu1, newdata=dff.test))
#   IF_N_X_pi_hat = as.numeric(predict(IF_N_X_pi, newdata=dff.test))
#   
#   IF_N_X_mu0_hat = PI_N_X_mu0_hat
#   IF_N_X_mu1_hat = PI_N_X_mu1_hat
#   
#   # Defining my pseudo-outcome
#   mu0 = IF_N_X_mu0_hat
#   mu1 = IF_N_X_mu1_hat
#   pi = IF_N_X_pi_hat
#   A = dff$A
#   Y = dff$Y
#   
#   IF_N_X_ystar = (1/mu1)*((mu0/mu1)*(1/pi)*A*(Y-mu1) - (1/(1-pi))*(1-A)*(Y-mu0)) + (1 - mu0/mu1)
#   
#   # Fitting model
#   fun.obj = function(param, theY){
#     
#     beta1 = param[1]
#     beta2 = param[2]
#     beta3 = param[3]
#     beta4 = param[4]
#     beta5 = param[5]
#     beta6 = param[6]
#     beta7 = param[7]
#     beta8 = param[8]
#     beta = as.matrix(c(beta1, beta2, beta3, beta4, beta5, beta6, beta7, beta8))
#     
#     # change these for each estimator
#     X = as.matrix(dff[, 3:(dim(dff)[2])])
#     g = expit(X %*% beta)
#     
#     psi1 = mean( X[,1]*g*(1-g)*(theY - g) )
#     psi2 = mean( X[,2]*g*(1-g)*(theY - g) )
#     psi3 = mean( X[,3]*g*(1-g)*(theY - g) )
#     psi4 = mean( X[,4]*g*(1-g)*(theY - g) )
#     psi5 = mean( X[,5]*g*(1-g)*(theY - g) )
#     psi6 = mean( X[,6]*g*(1-g)*(theY - g) )
#     psi7 = mean( X[,7]*g*(1-g)*(theY - g) )
#     psi8 = mean( X[,8]*g*(1-g)*(theY - g) )
#     
#     psi_norm = sqrt( psi1^2 + psi2^2 + psi3^2 + psi4^2 + psi5^2 + psi6^2 + psi7^2 + psi8^2 )
#     return(-psi_norm)
#   }
#   
#   true_values = c(.1,.1,.1,.1,.1,.1,.1,.1)
#   result_optim = optim(f = fun.obj, p = true_values, theY = IF_N_X_ystar) #,control=list(trace=TRUE))
#   betahat = result_optim$par
#   betahat # [1]  2.53984331  0.82933033 -1.42309767 -0.02986857  0.89654234  2.37261168  2.23507364  0.29806193
#   round(exp(betahat),2) # [1] 12.68  2.29  0.24  0.97  2.45 10.73  9.35  1.35
#   
#   # Getting predicted values
#   X = as.matrix(dff[, 3:(dim(dff)[2])])
#   IF_N_X_gammahat = expit(X %*% betahat)
#   IF_N_X_gammahat_mypoint = expit(betahat[1]*mypoint[1] + betahat[2]*mypoint[2] + 
#                                     betahat[3]*mypoint[3] + betahat[4]*mypoint[4] +
#                                     betahat[5]*mypoint[5] + betahat[6]*mypoint[6] +
#                                     betahat[7]*mypoint[7] + betahat[8]*mypoint[8])
#   
#   return(IF_N_X_gammahat_mypoint)
# }
# 
# # apply bootstrap
# library(doParallel)
# registerDoParallel(4)
# 
# bootreps=1
# IF_N_X_bootvec = 0
# IF_N_X_bootvec.1 = foreach(i=1:bootreps, .options.multicore=list(preschedule=TRUE)) %dopar% {
#   
#   # randomize indices
#   index.b = sample(1:nrow(dff), replace=FALSE)
#   
#   # select new sample of rows from dff
#   newdff = dff[index.b,]
#   
#   # calculating gammahat_star (star because it's from a bootstrap)
#   IF_N_X_gammahat_star_mean = fun.IF_N_X_boot(newdff)
#   
#   # store in a vector
#   IF_N_X_bootvec[i] <- IF_N_X_gammahat_star_mean
# }
# 
# # Standard error
# IF_N_X_gammahat_sd_mypoint = sd(unlist(IF_N_X_bootvec.1), na.rm = TRUE)
# 
# 
# # Confidence interval
# IF_N_X_ci_mypoint = paste(IF_N_X_gammahat_mypoint - 2*IF_N_X_gammahat_sd_mypoint / sqrt(samplesize),
#                           IF_N_X_gammahat_mypoint + 2*IF_N_X_gammahat_sd_mypoint / sqrt(samplesize),
#                           sep=", ")


# variance
estgammahat = IF_N_X_gammahat
estystar = as.matrix(dff$Y)
estmod = betahat
X = as.matrix(dff[, 3:(dim(dff)[2])])
x = mypoint

beta1 = estmod[1]
beta2 = estmod[2]
beta3 = estmod[3]
beta4 = estmod[4]
beta5 = estmod[5]
beta6 = estmod[6]
beta7 = estmod[7]
beta8 = estmod[8]
beta = c(beta1, beta2, beta3, beta4, beta5, beta6, beta7, beta8)

g_mypoint = expit(x %*% beta)
g = expit(X %*% beta)

dgdbeta = x %*% (g_mypoint*(1-g_mypoint))

forphi1 = X[,1]*g*(1-g)*(estystar - g)
forphi2 = X[,2]*g*(1-g)*(estystar - g)
forphi3 = X[,3]*g*(1-g)*(estystar - g)
forphi4 = X[,4]*g*(1-g)*(estystar - g)
forphi1 = X[,5]*g*(1-g)*(estystar - g)
forphi2 = X[,6]*g*(1-g)*(estystar - g)
forphi3 = X[,7]*g*(1-g)*(estystar - g)
forphi4 = X[,8]*g*(1-g)*(estystar - g)

samplesize=nrow(dff)
phi.arr = array(0L, dim=c(8,samplesize))  # IS THIS RIGHT?
phi.arr[1, 1:samplesize] = forphi1
phi.arr[2, 1:samplesize] = forphi2
phi.arr[3, 1:samplesize] = forphi3
phi.arr[4, 1:samplesize] = forphi4
phi.arr[5, 1:samplesize] = forphi1
phi.arr[6, 1:samplesize] = forphi2
phi.arr[7, 1:samplesize] = forphi3
phi.arr[8, 1:samplesize] = forphi4
phi = as.matrix(phi.arr)

covphi = (1/samplesize)*(phi %*% t(phi))

phistar = mean(estystar + estgammahat)

fordphidbeta = (phistar*g - phistar*g^2 - 2*g^2 + 5*g^3 - 2*phistar*g^3 - 3*g^4)
forM1 = mean((mypoint[1]^2)*fordphidbeta)
forM2 = mean((mypoint[2]^2)*fordphidbeta)
forM3 = mean((mypoint[3]^2)*fordphidbeta)
forM4 = mean((mypoint[4]^2)*fordphidbeta)
forM5 = mean((mypoint[5]^2)*fordphidbeta)
forM6 = mean((mypoint[6]^2)*fordphidbeta)
forM7 = mean((mypoint[7]^2)*fordphidbeta)
forM8 = mean((mypoint[8]^2)*fordphidbeta)

M.arr = array(0L, dim=c(8,8))
M.arr[1,1] = forM1
M.arr[2,2] = forM2
M.arr[3,3] = forM3
M.arr[4,4] = forM4
M.arr[5,5] = forM5
M.arr[6,6] = forM6
M.arr[7,7] = forM7
M.arr[8,8] = forM8
M = as.matrix(M.arr)
M.inverse = solve(M)

forcovphi = g*x*phistar - (g^2)*(x-x*phistar) + (g^3)*x
forcovphi1 = forcovphi2 = forcovphi3 = forcovphi4 = forcovphi5 = forcovphi6 = forcovphi7 = forcovphi8 = forcovphi

covphi.arr = array(0L, dim=c(8,8))
covphi.arr[1,1] = mean(forcovphi1 %*% t(forcovphi1))
covphi.arr[2,2] = mean(forcovphi2 %*% t(forcovphi2))
covphi.arr[3,3] = mean(forcovphi3 %*% t(forcovphi3))
covphi.arr[4,4] = mean(forcovphi4 %*% t(forcovphi4))
covphi.arr[5,5] = mean(forcovphi5 %*% t(forcovphi5))
covphi.arr[6,6] = mean(forcovphi6 %*% t(forcovphi6))
covphi.arr[7,7] = mean(forcovphi7 %*% t(forcovphi7))
covphi.arr[8,8] = mean(forcovphi8 %*% t(forcovphi8))
covphi = as.matrix(covphi.arr)

sandwich.variance_mypoint = t(dgdbeta) %*% M.inverse %*% covphi %*% t(M.inverse) %*% dgdbeta

# sd
IF_N_X_gammahat_sd_mypoint = sqrt(sandwich.variance_mypoint)

# confidence interval
IF_N_X_ci_mypoint = paste(round(IF_N_X_gammahat_mypoint - 2*IF_N_X_gammahat_sd_mypoint / sqrt(samplesize),2),
                          round(IF_N_X_gammahat_mypoint + 2*IF_N_X_gammahat_sd_mypoint / sqrt(samplesize),2),
                          sep=", ")
#[1] "0.79, 0.93"
# [1] "0.23, 0.77"


mean(IF_N_X_foraveraging_LB)
mean(IF_N_X_foraveraging_UB)
mean(IF_N_X_foraveraging_gammahat)




# Confidence intervals for betas 
betacov = M.inverse %*% covphi %*% t(M.inverse)
betahat1_sd = sqrt(betacov[1,1])
betahat2_sd = sqrt(betacov[2,2])
betahat3_sd = sqrt(betacov[3,3])
betahat4_sd = sqrt(betacov[4,4])
betahat5_sd = sqrt(betacov[5,5])
betahat6_sd = sqrt(betacov[6,6])
betahat7_sd = sqrt(betacov[7,7])
betahat8_sd = sqrt(betacov[8,8])

ci_beta1 = paste(round(exp(betahat[1] - 2*betahat1_sd/sqrt(samplesize)), 2), round(exp(betahat[1] + 2*betahat1_sd/sqrt(samplesize)), 2), sep=", ")
ci_beta2 = paste(round(exp(betahat[2] - 2*betahat2_sd/sqrt(samplesize)), 2), round(exp(betahat[2] + 2*betahat2_sd/sqrt(samplesize)), 2), sep=", ")
ci_beta3 = paste(round(exp(betahat[3] - 2*betahat3_sd/sqrt(samplesize)), 2), round(exp(betahat[3] + 2*betahat3_sd/sqrt(samplesize)), 2), sep=", ")
ci_beta4 = paste(round(exp(betahat[4] - 2*betahat4_sd/sqrt(samplesize)), 2), round(exp(betahat[4] + 2*betahat4_sd/sqrt(samplesize)), 2), sep=", ")
ci_beta5 = paste(round(exp(betahat[5] - 2*betahat5_sd/sqrt(samplesize)), 2), round(exp(betahat[5] + 2*betahat5_sd/sqrt(samplesize)), 2), sep=", ")
ci_beta6 = paste(round(exp(betahat[6] - 2*betahat6_sd/sqrt(samplesize)), 2), round(exp(betahat[6] + 2*betahat6_sd/sqrt(samplesize)), 2), sep=", ")
ci_beta7 = paste(round(exp(betahat[7] - 2*betahat7_sd/sqrt(samplesize)), 2), round(exp(betahat[7] + 2*betahat7_sd/sqrt(samplesize)), 2), sep=", ")
ci_beta8 = paste(round(exp(betahat[8] - 2*betahat8_sd/sqrt(samplesize)), 2), round(exp(betahat[8] + 2*betahat8_sd/sqrt(samplesize)), 2), sep=", ")


IF_N_X_gammahat_mypoint
IF_N_X_ci_mypoint
c(ci_beta1, ci_beta2, ci_beta3, ci_beta4, ci_beta5, ci_beta6, ci_beta7, ci_beta8)
round(exp(betahat),2)


# [1] "3.24, 3.65"   "6.95, 7.04"   "0.28, 0.28"   "16.79, 17.02" "35.02, 39.45" "22.9, 23.21"  "4.87, 5.49" "0.13, 0.13"  



# How does gammahat change for different values of mother's years of education?
varyinggammahat = c()
momeduc_range = seq(0,12,1)
for(i in momeduc_range){
  apoint = c(1, 1, i, 1, 1, 1, 1, 1)
  gammahat_apoint = expit(betahat[1]*apoint[1] + betahat[2]*apoint[2] + 
                            betahat[3]*apoint[3] + betahat[4]*apoint[4] +
                            betahat[5]*apoint[5] + betahat[6]*apoint[6] +
                            betahat[7]*apoint[7] + betahat[8]*apoint[8])
  varyinggammahat[i] = gammahat_apoint
}

pdf(file="pcvsmothereduc.pdf", width=6, height=5)
plot(varyinggammahat, type="o", main="Estimated PC v. mother's years of education", ylab="Estimated PC", xlab="Mother's years of education")
dev.off()


# How does gammahat change for different values of age?
varyinggammahat1 = c()
age_range = seq(0,17,1)
for(i in age_range){
  apoint = c(1, i, 1, 1, 1, 1, 1, 1)
  gammahat_apoint = expit(betahat[1]*apoint[1] + betahat[2]*apoint[2] + 
                            betahat[3]*apoint[3] + betahat[4]*apoint[4] +
                            betahat[5]*apoint[5] + betahat[6]*apoint[6] +
                            betahat[7]*apoint[7] + betahat[8]*apoint[8])
  varyinggammahat1[i] = gammahat_apoint
}
varyinggammahat1

pdf(file="pcvsage.pdf", width=6, height=5)
plot(varyinggammahat1, type="o", main="Estimated PC v. age", ylab="Estimated PC", xlab="Age")
dev.off()



# How does gammahat change for different values of gender?
apoint = c(0, 1, 1, 1, 1, 1, 1, 1)
gammahat_apoint.1 = expit(betahat[1]*apoint[1] + betahat[2]*apoint[2] + 
                            betahat[3]*apoint[3] + betahat[4]*apoint[4] +
                            betahat[5]*apoint[5] + betahat[6]*apoint[6] +
                            betahat[7]*apoint[7] + betahat[8]*apoint[8])

apoint = c(1, 1, 1, 1, 1, 1, 1, 1)
gammahat_apoint.2 = expit(betahat[1]*apoint[1] + betahat[2]*apoint[2] + 
                            betahat[3]*apoint[3] + betahat[4]*apoint[4] +
                            betahat[5]*apoint[5] + betahat[6]*apoint[6] +
                            betahat[7]*apoint[7] + betahat[8]*apoint[8])

gammahat_apoint.1 # 0.9755818
gammahat_apoint.2 # 0.9973464



